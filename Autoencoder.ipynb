{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahpibwofnTZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "import torch.utils.data\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.functional import pad\n",
        "from torch.nn.modules import Module\n",
        "from torch.nn.modules.utils import _single, _pair, _triple\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import functional as F\n",
        "import torch.backends.cudnn as cudnn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jx6iswVJB8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def umeyama(src, dst, estimate_scale):\n",
        "  \n",
        "    num = src.shape[0]\n",
        "    dim = src.shape[1]\n",
        "\n",
        "  \n",
        "    src_mean = src.mean(axis=0)\n",
        "    dst_mean = dst.mean(axis=0)\n",
        "\n",
        "  \n",
        "    src_demean = src - src_mean\n",
        "    dst_demean = dst - dst_mean\n",
        "\n",
        " \n",
        "    A = np.dot(dst_demean.T, src_demean) / num\n",
        "\n",
        "    d = np.ones((dim,), dtype=np.double)\n",
        "    if np.linalg.det(A) < 0:\n",
        "        d[dim - 1] = -1\n",
        "\n",
        "    T = np.eye(dim + 1, dtype=np.double)\n",
        "\n",
        "    U, S, V = np.linalg.svd(A)\n",
        "\n",
        "    rank = np.linalg.matrix_rank(A)\n",
        "    if rank == 0:\n",
        "        return np.nan * T\n",
        "    elif rank == dim - 1:\n",
        "        if np.linalg.det(U) * np.linalg.det(V) > 0:\n",
        "            T[:dim, :dim] = np.dot(U, V)\n",
        "        else:\n",
        "            s = d[dim - 1]\n",
        "            d[dim - 1] = -1\n",
        "            T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V))\n",
        "            d[dim - 1] = s\n",
        "    else:\n",
        "        T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V.T))\n",
        "\n",
        "    if estimate_scale:\n",
        "    \n",
        "        scale = 1.0 / src_demean.var(axis=0).sum() * np.dot(S, d)\n",
        "    else:\n",
        "        scale = 1.0\n",
        "\n",
        "    T[:dim, dim] = dst_mean - scale * np.dot(T[:dim, :dim], src_mean.T)\n",
        "    T[:dim, :dim] *= scale\n",
        "\n",
        "    return T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-NhndjO2rLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_transpose_axes(n):\n",
        "    if n % 2 == 0:\n",
        "        y_axes = list(range(1, n - 1, 2))\n",
        "        x_axes = list(range(0, n - 1, 2))\n",
        "    else:\n",
        "        y_axes = list(range(0, n - 1, 2))\n",
        "        x_axes = list(range(1, n - 1, 2))\n",
        "    return y_axes, x_axes, [n - 1]\n",
        "\n",
        "\n",
        "def stack_images(images):\n",
        "    images_shape = numpy.array(images.shape)\n",
        "    new_axes = get_transpose_axes(len(images_shape))\n",
        "    new_shape = [numpy.prod(images_shape[x]) for x in new_axes]\n",
        "    return numpy.transpose(\n",
        "        images,\n",
        "        axes=numpy.concatenate(new_axes)\n",
        "    ).reshape(new_shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOYimZvR20P6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_training_data(images, batch_size):\n",
        "    indices = numpy.random.randint(len(images), size=batch_size)\n",
        "    for i, index in enumerate(indices):\n",
        "        image = images[index]\n",
        "        image = random_transform(image, **random_transform_args)\n",
        "        warped_img, target_img = random_warp(image)\n",
        "\n",
        "        if i == 0:\n",
        "            warped_images = numpy.empty((batch_size,) + warped_img.shape, warped_img.dtype)\n",
        "            target_images = numpy.empty((batch_size,) + target_img.shape, warped_img.dtype)\n",
        "\n",
        "        warped_images[i] = warped_img\n",
        "        target_images[i] = target_img\n",
        "\n",
        "    return warped_images, target_images\n",
        "\n",
        "\n",
        "\n",
        "random_transform_args = {'rotation_range': 10,'zoom_range': 0.05,'shift_range': 0.05,'random_flip': 0.4,}\n",
        "\n",
        "\n",
        "def random_transform(image, rotation_range, zoom_range, shift_range, random_flip):\n",
        "    h, w = image.shape[0:2]\n",
        "    rotation = numpy.random.uniform(-rotation_range, rotation_range)\n",
        "    scale = numpy.random.uniform(1 - zoom_range, 1 + zoom_range)\n",
        "    tx = numpy.random.uniform(-shift_range, shift_range) * w\n",
        "    ty = numpy.random.uniform(-shift_range, shift_range) * h\n",
        "    mat = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n",
        "    mat[:, 2] += (tx, ty)\n",
        "    result = cv2.warpAffine(image, mat, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
        "    if numpy.random.random() < random_flip:\n",
        "        result = result[:, ::-1]\n",
        "    return result\n",
        "\n",
        "\n",
        "def random_warp(image):\n",
        "    assert image.shape == (256, 256, 3)\n",
        "    range_ = numpy.linspace(128 - 80, 128 + 80, 5)\n",
        "    mapx = numpy.broadcast_to(range_, (5, 5))\n",
        "    mapy = mapx.T\n",
        "\n",
        "    mapx = mapx + numpy.random.normal(size=(5, 5), scale=5)\n",
        "    mapy = mapy + numpy.random.normal(size=(5, 5), scale=5)\n",
        "\n",
        "    interp_mapx = cv2.resize(mapx, (80, 80))[8:72, 8:72].astype('float32')\n",
        "    interp_mapy = cv2.resize(mapy, (80, 80))[8:72, 8:72].astype('float32')\n",
        "\n",
        "    warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)\n",
        "\n",
        "    src_points = numpy.stack([mapx.ravel(), mapy.ravel()], axis=-1)\n",
        "    dst_points = numpy.mgrid[0:65:16, 0:65:16].T.reshape(-1, 2)\n",
        "    mat = umeyama(src_points, dst_points, True)[0:2]\n",
        "\n",
        "    target_image = cv2.warpAffine(image, mat, (64, 64))\n",
        "\n",
        "    return warped_image, target_image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRIkL0Sm3BL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class _ConvNd(Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, transposed, output_padding, groups, bias):\n",
        "        super(_ConvNd, self).__init__()\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.transposed = transposed\n",
        "        self.output_padding = output_padding\n",
        "        self.groups = groups\n",
        "        if transposed:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                in_channels, out_channels // groups, *kernel_size))\n",
        "        else:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                out_channels, in_channels // groups, *kernel_size))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        n = self.in_channels\n",
        "        for k in self.kernel_size:\n",
        "            n *= k\n",
        "        stdv = 1 / math.sqrt(n)\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = ('{name}({in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
        "             ', stride={stride}')\n",
        "        if self.padding != (0,) * len(self.padding):\n",
        "            s += ', padding={padding}'\n",
        "        if self.dilation != (1,) * len(self.dilation):\n",
        "            s += ', dilation={dilation}'\n",
        "        if self.output_padding != (0,) * len(self.output_padding):\n",
        "            s += ', output_padding={output_padding}'\n",
        "        if self.groups != 1:\n",
        "            s += ', groups={groups}'\n",
        "        if self.bias is None:\n",
        "            s += ', bias=False'\n",
        "        s += ')'\n",
        "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
        "\n",
        "\n",
        "class Conv2d(_ConvNd):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=True):\n",
        "        kernel_size = _pair(kernel_size)\n",
        "        stride = _pair(stride)\n",
        "        padding = _pair(padding)\n",
        "        dilation = _pair(dilation)\n",
        "        super(Conv2d, self).__init__(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
        "            False, _pair(0), groups, bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return conv2d_same_padding(input, self.weight, self.bias, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "# custom con2d, because pytorch don't have \"padding='same'\" option.\n",
        "def conv2d_same_padding(input, weight, bias=None, stride=1, padding=1, dilation=1, groups=1):\n",
        "\n",
        "    input_rows = input.size(2)\n",
        "    filter_rows = weight.size(2)\n",
        "    effective_filter_size_rows = (filter_rows - 1) * dilation[0] + 1\n",
        "    out_rows = (input_rows + stride[0] - 1) // stride[0]\n",
        "    padding_needed = max(0, (out_rows - 1) * stride[0] + effective_filter_size_rows -\n",
        "                  input_rows)\n",
        "    padding_rows = max(0, (out_rows - 1) * stride[0] +\n",
        "                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n",
        "    rows_odd = (padding_rows % 2 != 0)\n",
        "    padding_cols = max(0, (out_rows - 1) * stride[0] +\n",
        "                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n",
        "    cols_odd = (padding_rows % 2 != 0)\n",
        "\n",
        "    if rows_odd or cols_odd:\n",
        "        input = pad(input, [0, int(cols_odd), 0,int(rows_odd)])\n",
        "\n",
        "    return F.conv2d(input, weight, bias, stride,\n",
        "                  padding=(padding_rows // 2, padding_cols // 2),\n",
        "                  dilation=dilation, groups=groups)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b5a8_UW3557",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Architecture\n",
        "#=========================\n",
        "def toTensor(img):\n",
        "    img = torch.from_numpy(img.transpose((0, 3, 1, 2)))\n",
        "    return img\n",
        "\n",
        "\n",
        "def var_to_np(img_var):\n",
        "    return img_var.data.cpu().numpy()\n",
        "\n",
        "\n",
        "class _ConvLayer(nn.Sequential):\n",
        "    def __init__(self, input_features, output_features):\n",
        "        super(_ConvLayer, self).__init__()\n",
        "        self.add_module('conv2', Conv2d(input_features, output_features,\n",
        "                                        kernel_size=5, stride=2))\n",
        "        self.add_module('leakyrelu', nn.LeakyReLU(0.1, inplace=True))\n",
        "\n",
        "\n",
        "class _UpScale(nn.Sequential):\n",
        "    def __init__(self, input_features, output_features):\n",
        "        super(_UpScale, self).__init__()\n",
        "        self.add_module('conv2_', Conv2d(input_features, output_features * 4,\n",
        "                                         kernel_size=3))\n",
        "        self.add_module('leakyrelu', nn.LeakyReLU(0.1, inplace=True))\n",
        "        self.add_module('pixelshuffler', _PixelShuffler())\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input.view(input.size(0), -1)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Reshape(nn.Module):\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input.view(-1, 1024, 4, 4)  # channel * 4 * 4\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class _PixelShuffler(nn.Module):\n",
        "    def forward(self, input):\n",
        "        batch_size, c, h, w = input.size()\n",
        "        rh, rw = (2, 2)\n",
        "        oh, ow = h * rh, w * rw\n",
        "        oc = c // (rh * rw)\n",
        "        out = input.view(batch_size, rh, rw, oc, h, w)\n",
        "        out = out.permute(0, 3, 4, 1, 5, 2).contiguous()\n",
        "        out = out.view(batch_size, oc, oh, ow)  # channel first\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            _ConvLayer(3, 128),\n",
        "            _ConvLayer(128, 256),\n",
        "            _ConvLayer(256, 512),\n",
        "            _ConvLayer(512, 1024),\n",
        "            Flatten(),\n",
        "            nn.Linear(1024 * 4 * 4, 1024),\n",
        "            nn.Linear(1024, 1024 * 4 * 4),\n",
        "            Reshape(),\n",
        "            _UpScale(1024, 512),\n",
        "        )\n",
        "\n",
        "        self.decoder_A = nn.Sequential(\n",
        "            _UpScale(512, 256),\n",
        "            _UpScale(256, 128),\n",
        "            _UpScale(128, 64),\n",
        "            Conv2d(64, 3, kernel_size=5, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        self.decoder_B = nn.Sequential(\n",
        "            _UpScale(512, 256),\n",
        "            _UpScale(256, 128),\n",
        "            _UpScale(128, 64),\n",
        "            Conv2d(64, 3, kernel_size=5, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, select='A'):\n",
        "        if select == 'A':\n",
        "            out = self.encoder(x)\n",
        "            out = self.decoder_A(out)\n",
        "        else:\n",
        "            out = self.encoder(x)\n",
        "            out = self.decoder_B(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aKgWsjKfcIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_paths(directory):\n",
        "    return [x.path for x in os.scandir(directory) if x.name.endswith(\".jpg\") or x.name.endswith(\".png\")\n",
        "            or x.name.endswith(\".JPG\")]\n",
        "\n",
        "\n",
        "def load_images(image_paths, convert=None):\n",
        "    iter_all_images = (cv2.resize(cv2.imread(fn), (256, 256)) for fn in image_paths)\n",
        "    if convert:\n",
        "        iter_all_images = (convert(img) for img in iter_all_images)\n",
        "    for i, image in enumerate(iter_all_images):\n",
        "        if i == 0:\n",
        "            all_images = numpy.empty((len(image_paths),) + image.shape, dtype=image.dtype)\n",
        "        all_images[i] = image\n",
        "    return all_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B89RcXqT4f5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train_1\n",
        "#==========================\n",
        "print('===> Train init')\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "print('===> Loading datasets')\n",
        "images_A = get_image_paths(\"/content/drive/My Drive/data/trump\")\n",
        "images_B = get_image_paths(\"/content/drive/My Drive/data/cage\")\n",
        "images_A = load_images(images_A) / 255.0\n",
        "images_B = load_images(images_B) / 255.0\n",
        "#images_A += images_B.mean(axis=(0, 1, 2)) - images_A.mean(axis=(0, 1, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84l3-PsG5wFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train_2\n",
        "#==========================\n",
        "model = Autoencoder().double()\n",
        "\n",
        "print('===> Try resume from checkpoint')\n",
        "if os.path.isdir('drive'):\n",
        "    print('==================')\n",
        "    try:\n",
        "        checkpoint = torch.load('./drive/My Drive/data/checkpoint/autoencoder2.t7')\n",
        "        model.load_state_dict(checkpoint['state'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        print('===> Load last checkpoint data')\n",
        "    except FileNotFoundError:\n",
        "        print('Can\\'t found autoencoder2.t7')\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    print('===> Start from scratch')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqvYaovf4UmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(model.float(),(3,64,64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZqKdMzU4BdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train_2\n",
        "#==========================\n",
        "print('===> Training')\n",
        "criterion = nn.L1Loss()\n",
        "optimizer_1 = optim.Adam([{'params': model.encoder.parameters()},\n",
        "                          {'params': model.decoder_A.parameters()}]\n",
        "                         , lr=5e-5, betas=(0.5, 0.999))\n",
        "optimizer_2 = optim.Adam([{'params': model.encoder.parameters()},\n",
        "                          {'params': model.decoder_B.parameters()}]\n",
        "                         , lr=5e-5, betas=(0.5, 0.999))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print('Start training, press \\'q\\' to stop')\n",
        "\n",
        "    for epoch in range(start_epoch, 5000):\n",
        "        batch_size = 26\n",
        "\n",
        "        warped_A, target_A = get_training_data(images_A, batch_size)\n",
        "        warped_B, target_B = get_training_data(images_B, batch_size)\n",
        "\n",
        "        warped_A, target_A = toTensor(warped_A), toTensor(target_A)\n",
        "        warped_B, target_B = toTensor(warped_B), toTensor(target_B)\n",
        "\n",
        "\n",
        "\n",
        "        optimizer_1.zero_grad()\n",
        "        optimizer_2.zero_grad()\n",
        "\n",
        "        warped_A = model(warped_A, 'A')\n",
        "        warped_B = model(warped_B, 'B')\n",
        "\n",
        "        loss1 = criterion(warped_A, target_A)\n",
        "        loss2 = criterion(warped_B, target_B)\n",
        "        loss = loss1.item() + loss2.item()\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        optimizer_1.step()\n",
        "        optimizer_2.step()\n",
        "        print('epoch: {}, lossA:{}, lossB:{}'.format(epoch, loss1.item(), loss2.item()))\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "\n",
        "            test_A_ = target_A[0:14]\n",
        "            test_B_ = target_B[0:14]\n",
        "            test_A = var_to_np(target_A[0:14])\n",
        "            test_B = var_to_np(target_B[0:14])\n",
        "            print('===> Saving models...')\n",
        "            state = {\n",
        "                'state': model.state_dict(),\n",
        "                'epoch': epoch\n",
        "            }\n",
        "            if not os.path.isdir('./drive/My Drive/data/checkpoint'):\n",
        "                os.mkdir('checkpoint')\n",
        "            torch.save(state, './drive/My Drive/data/checkpoint/autoencoder2.t7')\n",
        "\n",
        "        figure_A = np.stack([\n",
        "            test_A,\n",
        "            var_to_np(model(test_A_, 'A')),\n",
        "            var_to_np(model(test_A_, 'B')),\n",
        "        ], axis=1)\n",
        "        figure_B = np.stack([\n",
        "            test_B,\n",
        "            var_to_np(model(test_B_, 'B')),\n",
        "            var_to_np(model(test_B_, 'A')),\n",
        "        ], axis=1)\n",
        "\n",
        "        figure = np.concatenate([figure_A, figure_B], axis=0)\n",
        "        figure = figure.transpose((0, 1, 3, 4, 2))\n",
        "        figure = figure.reshape((4, 7) + figure.shape[1:])\n",
        "        figure = stack_images(figure)\n",
        "\n",
        "        figure = np.clip(figure * 255, 0, 255).astype('uint8')\n",
        "\n",
        "        cv2.imwrite('./drive/My Drive/data/checkpoint/autoencoder.jpg', figure)\n",
        "        key = cv2.waitKey(1)\n",
        "        if key == ord('q'):\n",
        "            exit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7jUun6vrxXc",
        "colab_type": "code",
        "outputId": "9621efc6-ddb1-447c-eabb-ca013c5f6f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Сonvert video\n",
        "#=====================================\n",
        "import dlib\n",
        "\n",
        "video_name = './drive/My Drive/data/test/trump.mp4'\n",
        "video_path = os.path.join(os.path.realpath('.'), video_name)\n",
        "\n",
        "def extract_face(frame):\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    img = frame\n",
        "    dets = detector(img, 1)\n",
        "    for idx, face in enumerate(dets):\n",
        "        position = {}\n",
        "        position['left'] = face.left()\n",
        "        position['top'] = face.top()\n",
        "        position['right'] = face.right()\n",
        "        position['bot'] = face.bottom()\n",
        "        croped_face = img[position['top']:position['bot'], position['left']:position['right']]\n",
        "\n",
        "        return position, croped_face\n",
        "\n",
        "\n",
        "def extract_faces(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    n = 0\n",
        "    while (cap.isOpened() and n<1):\n",
        "        _, frame = cap.read()\n",
        "        fframe=cv2.imread('./drive/My Drive/data/test/trump_pic.jpg')\n",
        "        print(fframe.shape)\n",
        "        position, croped_face= extract_face(fframe)\n",
        "        print(croped_face.shape)\n",
        "        converted_face = convert_face(croped_face)\n",
        "        converted_face = converted_face.squeeze(0)\n",
        "        converted_face = var_to_np(converted_face)\n",
        "        converted_face = converted_face.transpose(1,2,0)\n",
        "        converted_face = np.clip(converted_face * 255, 0, 255).astype('uint8')\n",
        "        rrr=cv2.resize(converted_face, (256,256))\n",
        "        cv2.imwrite('./drive/My Drive/data/checkpoint/warp2.jpg',rrr )\n",
        "        cv2.waitKey(2000)\n",
        "        back_size = cv2.resize(converted_face, (croped_face.shape[0]-10, croped_face.shape[1]-10))\n",
        "        merged = merge(position, back_size, fframe)\n",
        "        cv2.imwrite('./drive/My Drive/data/checkpoint/warp2.jpg',merged )\n",
        "        out.write(merged)\n",
        "        n = n + 1\n",
        "        print(n)\n",
        "\n",
        "def convert_face(croped_face):\n",
        "    resized_face = cv2.resize(croped_face, (256, 256))\n",
        "    normalized_face = resized_face / 255.0\n",
        "    warped_img, _ = random_warp(normalized_face)\n",
        "    batch_warped_img = np.expand_dims(warped_img, axis=0)\n",
        "\n",
        "    batch_warped_img = toTensor(batch_warped_img)\n",
        "    \n",
        "    model = Autoencoder().double()\n",
        "    checkpoint = torch.load('./drive/My Drive/data/checkpoint/autoencoder2.t7')\n",
        "    model.load_state_dict(checkpoint['state'])\n",
        "\n",
        "    converted_face = model(batch_warped_img,'B')\n",
        "\n",
        "    return converted_face\n",
        "\n",
        "def merge(postion, face, body):\n",
        "    mask = 255 * np.ones(face.shape, face.dtype)\n",
        "    width, height, channels = body.shape\n",
        "    center = (postion['left']+(postion['right']-postion['left'])//2, postion['top']+(postion['bot']-postion['top'])//2)\n",
        "    normal_clone = cv2.seamlessClone(face, body, mask, center, cv2.NORMAL_CLONE)\n",
        "    return normal_clone\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(video_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
        "    out = cv2.VideoWriter(\"me4_out.avi\", fourcc, 3, (1920, 1080))\n",
        "    extract_faces(video_path)\n",
        "\n",
        "    out.release()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/./drive/My Drive/data/test/trump.mp4\n",
            "(338, 600, 3)\n",
            "(90, 89, 3)\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwpLrC8XSjbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Video split\n",
        "#================================\n",
        "\n",
        "Video_Path = 'train/me2.mp4'\n",
        "video_Path = os.path.join(os.path.realpath('.'), Video_Path)\n",
        "save_path = os.path.join(os.path.dirname(video_Path), 'me/')\n",
        "\n",
        "cap = cv2.VideoCapture(video_Path)\n",
        "n=0\n",
        "fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "out = cv2.VideoWriter('v_out.avi', fourcc, 10, (frame_width, frame_height))\n",
        "while (cap.isOpened()) and n<500:\n",
        "    ret, frame = cap.read()\n",
        "    #frame = frame.reshape(frame.shape[1],frame.shape[0],3)\n",
        "    save_images = os.path.join(save_path, str(n)+'.jpg')\n",
        "    cv2.imwrite(save_images, frame)\n",
        "    # n = n + 1\n",
        "    if ret==True:\n",
        "        #out.write(frame)\n",
        "        cv2.imshow('frame', frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    n = n +1\n",
        "    print(n)\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2FMgNK2TvO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Face split\n",
        "#=====================================\n",
        "\n",
        "Images_Folder = 'train/me'\n",
        "OutFace_Folder = 'train/me_face/'\n",
        "\n",
        "Images_Path = os.path.join(os.path.realpath('.'), Images_Folder)\n",
        "\n",
        "pictures = os.listdir(Images_Path)\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "print(pictures)\n",
        "\n",
        "def rotate(img):\n",
        "    rows,cols,_ = img.shape\n",
        "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), -90, 1)\n",
        "    dst = cv2.warpAffine(img, M, (cols, rows))\n",
        "    return dst\n",
        "\n",
        "for f in pictures:\n",
        "    img = cv2.imread(os.path.join(Images_Path,f), cv2.IMREAD_COLOR)\n",
        "    b, g, r = cv2.split(img)\n",
        "    img2 = cv2.merge([r, g, b])\n",
        "    img = rotate(img)\n",
        "\n",
        "    dets = detector(img, 1)\n",
        "    \n",
        "\n",
        "    for idx, face in enumerate(dets):\n",
        "     \n",
        "        left = face.left()\n",
        "        top = face.top()\n",
        "        right = face.right()\n",
        "        bot = face.bottom()\n",
        "        crop_img = img[top:bot, left:right]\n",
        "        cv2.imwrite(OutFace_Folder+f[:-4]+\"_face.jpg\", crop_img)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}